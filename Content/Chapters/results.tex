\chapter{Results}
\label{sec:results}

This chapter will present the empirical results
of the branch-and-cut algorithm,
whose implementation was discussed in \cref{sec:implementation-chapter}.
We will evaluate its performance as a pricer for the CVRP,
by comparing it with
the state-of-the-art labeling algorithm based on dynamic programming
discussed in the works of \textcite{pessoa2020generic, sadykov2021bucket}.

\medskip

\section{CVRP Benchmark Instances}
\label{sec:results-benchmark-instances}

Several CVRP benchmark instances are mandated to measure the competitiveness
of our branch-an-cut pricer in an accurate manner.
The \urlref{http://vrp.galgos.inf.puc-rio.br/index.php/en/}{CVRPLIB website}
is an online database for the vehicle routing problem that includes
several downloadable test instances for free, among other things.
It is a valuable resource for all practitioners interested in the VRP.
It includes interactive plots of various routing problem instances
and the optimal (or best known) solution discovered by the best scholars over time.
Each CVRP test instance is stored in a file with the following filename template:
\begin{center}
	\begin{LVerbatim}
		<F>-n<N>-k<K>.vrp
	\end{LVerbatim}
\end{center}
where \texttt{.vrp} is the file extension,
\texttt{<N>} is the number of vertices in the instance, and
\texttt{<K>} is the number of (maximum or exact) available vehicles.
Finally, \texttt{<F>} represents the set instance family.
\texttt{<F>} is a single letter that uniquely identifies the instance
set and the authors who published such a set.
\texttt{P-n40-k5.vrp}, for example, denotes a test instance made up
of $40$ nodes ($39$ customers) and $5$ vehicles.
The \texttt{P} in the name refers to the instance set family,
which was published in \textcite{augerat1995}.

Each CVRP instance file's contents adhere
to the \texttt{TSPLIB95} file format \parencite{reinelt1995}.
Distances between pairs of nodes are computed
using the 2D Euclidean distance function rounded to the nearest integer,
as became standard for the TSP in \textcite{reinelt1991}.
Rounding is performed to stabilize the optimal values,
allowing for an accurate comparison of different contributions.
However, rounding introduces issues when comparing heuristic contributions;
for more information, see \textcite{uchoa2017}.

\medskip

We used some of the most historic and well-known CVRP instances to evaluate the performance of our pricer:
set \texttt{A}, \texttt{B}, \texttt{P} \parencite{augerat1995},
set \texttt{E} \parencite{dantzig1959, christofides1969, gaskell1967bases, gillett1974heuristic},
and finally set \texttt{F} \parencite{fisher1994}.
If the reader is wondering how these test sets were generated in the first place,
they can consult the CVRPLIB website or the work of \textcite{uchoa2017}.

The employed test instances are summarized in
\cref{table:cvrp-instance-family-E,table:cvrp-instance-family-F,table:cvrp-instance-family-A,table:cvrp-instance-family-B,table:cvrp-instance-family-P}.

\input{Content/Chapters/Include/employed-cvrp-instance-tables.tex}

\subsection{Inflation of the CVRP Test Instances}
\label{sec:inflation-of-the-cvrp-test-instances}

We created new artificial instances based on the presented sets in
\cref{table:cvrp-instance-family-E,table:cvrp-instance-family-F,table:cvrp-instance-family-A,table:cvrp-instance-family-B,table:cvrp-instance-family-P}.
The idea is to generate new CVRP instances characterized by longer routes
so that we can evaluate the behavior of the proposed branch-and-cut pricer and labeling algorithm
as the routes they need to produce grow longer (see previous discussions in
\cref{sec:intro-thesis-contributions,sec:solving-the-pricing-problem}).
The new artificial instances were created as follows.
Let $Q \in R_+,\ k \in Z_+$ respectively denote the vehicle
capacity and the number of trucks of the unmodified CVRP instance.
Define a scale factor $s \in R_+$.
For each unmodified CVRP instance, we generate a new artificial instance
characterized by the vehicle capacity $Q^\prime \in R_+$
and by the number of vehicles $k^\prime \in Z_+$.
We've inflated the total number of available CVRP instances
by using the following relationship:
$$
	Q^\prime = Q \times s, \quad K^\prime = \ceil*{\frac{K}{s}}
$$
Inflation is performed for each unmodified CVRP instance by using
multiple values for the scale factor $s \in R_+$.
As $s \in R_+$ grows, the inflated CVRP instances will look more and more like a conventional TSP.
If $\ceil*{\frac{K}{s}} = 1$ the CVRP decades to a TSP and, as a result,
the BAP frameworks may become a suboptimal approach for dealing with such a problem.

\section{\bapcod{}}
\label{sec:results-bapcod}

\bapcod{} \parencite{sadykov2021} is a software package
developed in France at the Bordeaux University and Bordeaux Research Center
that embeds a sophisticated column generation approach
embedded in a generic and modern branch-price-and-cut (BPC) algorithm.
\bapcod{} takes a compact mixed-integer programming model as input
and solves it using a Dantzig-Wolfe reformulation \parencite{dantzig1960}.
See the previous discussion on \cref{sec:set-partitioning-formulation,sec:branch-and-price}.
The modern BPC solver automatically applies the Dantzig-Wolfe reformulation.
In this case, a default generic pricer based on a MIP optimizer
is employed during the column generation phase.
\bapcod{} uses an automatic dual price smoothing stabilization,
as discussed in \textcite{pessoa2018automation},
to improve the convergence speed of the column generation.
\bapcod{} supports direct branching on the master formulation's arc-flow variables
as well as Vanderbeck branching \parencite{vanderbeck2011}.
The latter is a non-robust branching scheme imposing bounds modifications on the sub-problem variables.
Because \bapcod{} is generic, it supports user-developed extensions
which can alter the BPC algorithm's behaviour.
These extensions can provide the BPC framework with
custom-defined cutting planes (robust and non-robust),
custom branching decisions (robust and non-robust),
or even ad-hoc pricer implementations.

The \vrpsolver{} extension \parencite{pessoa2020generic}, is
a \bapcod{} extension distributed by the same authors.
This extension includes an
advanced implementation of a bidirectional dynamic programming labeling algorithm
\parencite{sadykov2021bucket} for solving the pricing problem.
The included labeling algorithm
can be used as an exact or heuristic pricer.
The labeling algorithm contains two successively lighter heuristic implementations;
for more information, see \textcite{sadykov2021bucket}.
The labeling algorithm makes use of a generalized ng-sets definition \parencite{baldacci2011}
defined through the \textit{packing} and \textit{elementarity} sets,
as described in \textcite{pessoa2020generic}.
When the solution obtained after the CG convergence is fractional,
the ng-sets are dynamically augmented,
see \textcite{pessoa2020generic} for more details.
The \vrpsolver{} extension, also,
includes the implementation of some
specific cutting planes and branching decisions
aimed at efficiently solving routing-like problems
(or problems that exhibit similar structures)
such as the CVRP, VRPTW, and also others (see \cite{pessoa2020generic}).
The \vrpsolver{} implements the robust RCC cuts of \textcite{laporte1983},
and the non-robust limited-memory rank-1 cuts of \textcite{pecin2017improved}.
The \vrpsolver{} also implements the non-robust Ryan\&Foster branching \parencite{ryan1981integer}.
The \vrpsolver{} extension was also used successfully in \textcite{pessoa2020}
to solve the Bin Packing Problem (BPP).

\medskip

Currently, \bapcod{} and its \vrpsolver{} extension are the leading cutting-edge technologies
for solving vehicle routing problems \parencite{pessoa2020generic}.
The \bapcod{} source code is available for free, for academic purposes only,
by issuing a form request to this URL: \url{https://bapcod.math.u-bordeaux.fr/}.
The \vrpsolver{} extension, on the other hand, is only available in compiled form,
and must be explicitly requested via email by contacting one of the
original authors: \url{mailto:ruslan.sadykov@inria.fr}.
The official technical report presented in \textcite{sadykov2021} contains instructions
for building, configuring, and using \bapcod{} and its \vrpsolver{} extension.
To round things out, a Julia language interface to the \vrpsolver{} extension
can be found at this \urlref{https://github.com/inria-UFF/BaPCodVRPSolver.jl}{Github repo}.

For a comprehensive technical discussion of the components of
modern and advanced BPC frameworks, see \textcite{sadykov2019modern}.

\section{Evaluation Setup}
\label{sec:results-evaluation-setup}

The goal of this thesis, as previously stated,
is to determine whether a branch-and-cut framework can be competitive in solving the pricing problem,
particularly as the produced routes grow in length.
We hypothesized that, since the labeling algorithm's performance degrades
as the vehicle capacity increases,
a branch-and-cut framework could aid in solving the more demanding pricing problems.
Refer back to the discussion in \cref{sec:intro-thesis-contributions,sec:solving-the-pricing-problem}
for additional details.

However, the two frameworks operate in very different ways.
Our branch-and-cut framework does not support non-robust inequalities or variable fixing
but produces stronger dual bounds.
Similarly, the labeling algorithm instead generates weaker dual bounds
but allows for multiple column generation per pricing invocation,
non-robust cut generations and branching.
As a result, comparing the performance of the two schemes is not an easy task.
So a natural question arises.
What is the best approach to compare the performance of the two frameworks?

\medskip

When it came to developing our branch-and-cut pricer, we had two options:
(i) developing our pricer in tight integration with the BPC framework, by coding it as a \bapcod{} plugin,
or (ii) developing the pricer in a standalone executable, separate from the BPC framework.

The first option has two primary benefits.
For starters, it enables our branch-and-cut pricer to influence the BPC algorithm's execution flow.
Second, it empowers more diversified ways of measuring the pricer's effectiveness.
We can decide to compare
the running time required for solving the root node, the running time for solving the entire CVRP instance
or the running time of each single pricing iteration.
The disadvantage of the first approach is its implementation complexity
and constraint in tooling or programming languages employed.
Programming the BAC pricer as \bapcod{} plugin requires heavy knowledge of this BPC algorithm's intricacies.

The second option, on the other hand, has one significant advantage:
it allows for greater implementation flexibility, simplifying development efforts.
We can develop the BAC pricer in complete isolation,
using whatever programming language or external library we want.
However, the branch-and-cut pricer cannot change the BPC algorithm's execution flow.
As a result, the branch-and-cut pricer follows the same execution flow
as the BPC algorithm  using the labeling algorithm.
This scenario led to two primary downsides.
For starters, improved dual-bounds from the BAC algorithm
will not affect the number of pricing iterations.
As a result, the BAC algorithm's solution to the more difficult elementary SPPRC
has no practical way to pay off its high computation times.
Secondly, the time required to solve each pricing iteration
is the only viable metric to measure the pricers' competitiveness.

\medskip

In the end, we decided on the second option.
We chose simplicity over complexity and built our pricer as a standalone executable.
Measuring the running time of each pricing iteration is neither an appropriate nor a poor approach.
There are both benefits and downsides to this approach.
First \bapcod{} must be configured to operate in the same domain as our pricer,
thereby disabling many features that could potentially result in high efficiency in the broader scheme.
Second, our branch-and-cut pricer generates elementary bounds, naturally solving a harsher problem.
While solving a single pricing iteration may take longer,
stronger dual-bounds may skip some pricing iterations in the long run,
thus resulting in an overall faster CVRP resolution.
On the other hand, measuring the pricers' efficiency per each single pricing iteration,
it is straightforward to assess compared to the alternative approaches described.

Summing up, it is clear that the performance comparison that we've settled on is just
a mere indicator of the genuine efficiency of the two approaches and must
therefore be taken cautiously.
We believe, however, there is still value in performing such a comparison as it provides
a first proof-of-concept/direction regarding the feasibility of the effectiveness
of a branch-and-cut scheme for addressing the pricing problem.

\medskip

We conclude this section by introducing the \bapcod{} parametrization,
which we used to bend the labeling algorithm to operate in a compatible environment for the BAC algorithm.

The configuration parameters are specified separately
in an appropriately placed configuration file.
The BPC framework's branching and cut-generation schemes have been disabled,
thus asking the algorithm to halt at the root node of the branch-and-bound tree.
We enabled the ng-sets' augmentation and set the maximum ng-set threshold as high as possible,
therefore putting pressure on the labeling algorithm to generate dual bounds as close
to the elementary bound as possible at the end of each CG iteration.
The pricer's tailing-off condition was disabled.
We've also asked the labeling algorithm to generate
a single column per pricing invocation.
Refer to \cref{sec:bapcod-appendix} for a complete list of the employed configuration.

\section{Evaluation Process}
\label{sec:results-evaluation-process}

We employed \bapcod{} version 0.66 (released in November 2021) and the \texttt{libRCSP v0.5.12}.
The latter library contains the \vrpsolver{} extension's implementation.
The goal is to use \bapcod{} to solve many CVRP instances while simultaneously
measuring the labeling algorithm's running at each pricing invocation.

\medskip

We re-adapted one of the VRPTW examples (included in the distribution)
to model a Capacitated Vehicle Routing Problem by following
the \bapcod{} technical document of \textcite{sadykov2021}.
The master formulation that we've implemented follows the two-index arc flow model
presented in \cref{eq:two-index-flow-obj-func,eq:two-index-flow-two-edges-incident-per-customer,eq:two-index-flow-two-k-edges-incident-in-the-depot-node,eq:two-index-flow-ccc,eq:two-index-flow-x-mip-var-bounds-depot,eq:two-index-flow-x-mip-var-bounds},
excluding the Rounded Capacity Constraints (RCC) of \cref{eq:two-index-flow-ccc}.

To use the \vrpsolver{} extension's labeling algorithm,
the associated Resource Constrained Shortest Path (RCSP) sub-problem must be defined.
The RCSP sub-problem is formulated on a complete directed network by
linking the new RCSP modeling variables to the master problem formulation.
The RCSP sub-problem necessitates the correct definition of the following:
the source/sink vertices, as well as the so-called \textit{packing} and \textit{elementarity} sets
(see \cite{pessoa2020generic} for more details).
The correct definition of these generalized sets is required
for specific components  of the \vrpsolver{} extension to function properly (ng-sets, RCC separation, etc).
For each customer, a unique \textit{packing} set and \textit{elementarity} set are created.
The \vrpsolver{} extension also necessitates the explicit definition of an additional
distance matrix encoding the distance between pairs of elementarity sets.
The elementarity sets and the distance matrix specified by the user
let \bapcod{} compute the ng-sets automatically.

Due to internal implementation details of \bapcod{},
the RCSP sub-problem requires the user to specify the consumptions (or demand) for each network's arc.
Resource consumptions cannot aren't definable at the vertex level.
Therefore, as suggested in \textcite{pessoa2020generic}, we have defined the resource consumption
on the arcs as $q_{ij} = \frac{q_{i} + q_{j}}{2} \quad \forall i, j \in V$.
This definition is valid and it leads to a symmetric resource consumption ($q_{ij} = q_{ji}$).
The symmetric resource consumption property improves the efficiency in pricing
by eliminating the need for backward labeling \parencite{pessoa2020generic}.

We implemented a custom pricing functor to extract the information
required for the performance evaluation.
The pricing functor is essentially a glorified callback
that can be used to solve the pricing problem through user-defined custom code.
Our pricing functor is a simple stubbed implementation
that ultimately invokes the pricing functor of the standard \vrpsolver{} extension.
However, before calling the labeling algorithm, we first do the following.
First, we assess the labeling algorithm's performance by measuring its running time.
Second, at each pricing invocation, we record the dual variables $\pi \in \R^N$ of the RMP
to be later dumped into a dedicated file.

Because of the implementation details of \bapcod{},
the reduced cost of an edge does not follow the simple relationship
$\bar{c}_{ij} = c_{ij} - \frac{\pi_i + \pi_j}{2} \quad \forall i, j \in V,\ i \ne j$.
By accessing an appropriate field on the MP formulation structure,
the user can retrieve the dual variables $\pi_0 \in R$.
The remaining dual variables $\pi_i \quad \forall i \in V_0$
must be calculated from the reduced cost of each edge $\bar{c}_{ij}$.
Internally, \bapcod{} encodes the reduced cost of an edge using the following relationship:
\begin{equation}
	\bar{c}_{ij} = \begin{cases}
		c_{ij} - \frac{\pi_{j}}{2}       & \texttt{if } i = 0, j \in V_0       \\
		c_{ij} - \frac{\pi_i + \pi_j}{2} & \texttt{if } i, j \in V_0,\ i \ne j
	\end{cases}
	\label{eq:bapcod-encoded-reduced-cost-vars}
\end{equation}
By exploiting \cref{eq:bapcod-encoded-reduced-cost-vars},
we've implemented a trivial recursive algorithm
to extract all the necessary dual variables $\pi \in \R^N$.

In conclusion, we save two files to the hard drive for each pricing invocation of the labeling algorithm.
The first file is a JSON file that contains metadata information about
the labeling algorithm, such as the instance name, the instance size,
the total running time for the labeling, the reduced cost of the generated route
and the column generation iteration.
The second file, instead, encodes a CPTP instance including:
the dual variables (the vertices' profit),
the original demands, the original node positions, and the vehicle capacity.
The CPTP instance is encoded in a slightly modified TSPLIB95 file format.
In the file format, we've added a new dedicated section, called \texttt{PROFIT\_SECTION},
to hold the extracted dual variables.

Only the last $10$ of all the pricing problems dumped to disk are kept for empirical evaluation.
As a result, for a fixed inflation scale factor $s$,
we totaled $70$ pricing instances from the \texttt{E} test set,
$30$ from the \texttt{F} test set,
$210$ from the \texttt{A} test set,
$190$ from the \texttt{B} test set
and $240$ from the \texttt{P} test set.

\subsection{Performance profiles}
\label{sec:results-performance-profiles}

\textit{Performance profiles}, first introduced in \textcite{dolan2002},
are a plot representation used to benchmark optimization software.
These novel plot representations are easy to interpret and less susceptible to personal interpretation.
In a nutshell, performance profiles represent the cumulative distribution function of a performance metric.
We put $H$ algorithms to the test by running them through $M$ problem instances.
The metric under consideration in \textit{performance profiles}
is typically the overall running time of an algorithm.
When the metric under measurement is the running time, we obtain the so-called \textit{time profiles}.

In \textit{time profiles}, the time ratio relative to a baseline is plotted on the X-axis.
The baseline is calculated as the best performance obtained from all the algorithms under consideration.
Instead, the Y-axis depicts the likelihood of being within an X ratio of the baseline.
In \textit{time profiles}, the ideal solver appears as a straight vertical line on the entire left of the plot.
An example of a \textit{time profile} is provided in \cref{fig:example-of-time-profile-tsp}.

In our case, we also employ the so-called \textit{cost profiles} to plot either the optimal value
or the best dual-bound found from each algorithm.
Instead of computing its ratio w.r.t. a baseline, the raw value of the metric is plotted directly.
Recording the dual bounds inside a \textit{cost profile}
allows us to select the best algorithm
based on its convergence speed over a limited time frame (time-limit).
When plotting the optimal value in the \textit{cost profiles}, on the other hand,
we can validate the pricer that would feed the BPC framework with tighter dual bounds.
Because the optimal value $z$ always satisfies $z \le 0$,
we used two sentinel values to accentuate some specific circumstances.
When a pricer successfully determines that no reduced cost route exists
($z > \varepsilon_{\mt{ct}}$),
we output $z = 1.0$ in the associated \textit{cost profile}.
Instead, if the pricer fails to solve the PP optimally within the timelimit,
we output $z = 2.0$.
For additional output clarity, in \textit{cost profiles} we tint
the plot's background color in green and red for $z \le \varepsilon_{\mt{ct}}$ and $z > \varepsilon_{\mt{ct}}$ respectively.

\medskip

In this thesis, we evaluated the efficiency of our proposed BAC-based pricer
using both the \textit{cost and time profiles}.
Our pricer's efficiency is compared to the RCSP dynamic programming algorithm
included in the \vrpsolver{} extension \parencite{pessoa2020generic} of \bapcod{}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{./Imgs/perfprof-tsp-example/time-ratio20.0.cropped.pdf}
	\caption{
		This example of a \textit{time profile} depicts two exact algorithms for the TSP under analysis.
		A vertical straight line on the entire left side of the plot would represent the best scenario.
		In the situation depicted in the figure, the algorithm named \texttt{FLOW-1|EXT\_CONST=1} outperforms the other algorithm in the vast majority of cases.
	}
	\label{fig:example-of-time-profile-tsp}
\end{figure}

\subsection{Performance Variability}
\label{sec:performance-variability}

\textit{Performance variability} \parencite{lodi2013performance} can be defined as an unexpected change in performance that is not the result of a genuine improvement.
There are numerous cases of variability, but one of the most common is randomness, specifically the initial state of random generators.
Resolution methods that rely on randomness may perform well by chance, and the effect is magnified for the so-called \textit{chaotic systems}.
A slight perturbation to the initial conditions has a tremendous impact on the evolution of a chaotic system.

Branch-and-cut algorithms are chaotic systems by nature.
Unlucky branching decisions, appearing at high levels of the branching tree, have an exponential impact on the size of the tree itself.

Performance profiles should filter out the noise and measure the genuine algorithmic improvements of a resolution method.
To avoid performance variabilities, we can upsample each test-set instance using a different initial random seed each time.
With each random seed, each test instance is solved multiple times.
The greater the number of seeds considered, the more likely the performance profile captures a genuine algorithmic improvement rather than variability.

Although using multiple seeds is generally good practice when testing chaotic systems, we used a single seed per pricing instance to substantially decrease the computation time required for the empirical evaluation.

\section{Empirical Results}
\label{sec:results-empirical-results}

The empirical results are presented in this section.
The results were obtained on a desktop computer equipped with
an AMD Ryzen 7 1700X 3.4GHz with 8 cores (16 virtual cores),
2x8 GBs of memory @ 3200 MHz (part number: CMK16GX4M2B3200C16),
running a GNU/Linux operating system with kernel v5.18.3 and Glibc v2.35.
For our BAC pricer's implementation, we used CPLEX version 22.1 (released in March 2022).

\medskip

We divide the empirical results into two groups.

\medskip

The first group is committed to testing various versions of the implemented BAC algorithm based on the aggressiveness used for the fractional labeling discussed in \cref{sec:impl-labeling-fractional-solutions}.
In the first version of the algorithm, we perform cutting-plane separation aggressively on each possible occasion.
We call this version of the algorithm \texttt{BAC EFL}, where \texttt{EFL} stands for Exhaustive Fractional Labeling.
\texttt{BAC EFL} computes the Gomory-Hu tree on each fractional solution.
The second version of the algorithm is known as \texttt{BAC AFL} (Amortized Fractional Labeling).
\texttt{BAC AFL} amortizes the cost of fractional labeling over multiple iterations.
Namely, fractional separations iterations that are not multiple of $N$ become a no-op, no min-cuts computations are performed, and thus no cutting-planes are separated.
In the last version of the algorithm, which we call \texttt{BAC NFL}, we perform no fractional labeling (\texttt{NFL} stands for No Fractional Labeling).
The cost of computing Gomory-Hu trees becomes zero, but cutting planes will never be separated for fractional solutions.

The computational evaluation of the first group is provided in \cref{fig:perfprofs-batch1-part1},
which contain solely cost profiles depicting the obtained dual bounds.
The results in \cref{fig:perfprofs-batch1-part1} were obtained using a single core and a time limit of 60 seconds.
The winner will be the BAC-pricer version that produces tighter bounds.
According to \cref{fig:perfprofs-batch1-part1}, the \texttt{BAC AFL} version produces tighter dual bounds in the majority of cases.
As a result, we will only compare the pricing efficiency of the \texttt{BAC AFL} version to the labeling algorithm in the second group of empirical results.

\medskip

The second group, on the other hand, compares our \texttt{BAC AFL} algorithm to the VRPSolver's labeling algorithm \parencite{pessoa2020generic}.
The computational evaluation of the second group is provided in
\cref{fig:perpfrofs-batch2-E-part1,fig:perpfrofs-batch2-E-part2,fig:perpfrofs-batch2-F-part1,fig:perpfrofs-batch2-F-part2,fig:perpfrofs-batch2-A-part1,fig:perpfrofs-batch2-A-part2,fig:perpfrofs-batch2-B-part1,fig:perpfrofs-batch2-B-part2,fig:perpfrofs-batch2-P-part1,fig:perpfrofs-batch2-P-part2}.
The BAC-pricer was run on 16 threads (corresponding to the virtual cores of the host machine) with a time limit of 20 minutes.
The empirical results of
\cref{fig:perpfrofs-batch2-E-part1,fig:perpfrofs-batch2-E-part2,fig:perpfrofs-batch2-F-part1,fig:perpfrofs-batch2-F-part2,fig:perpfrofs-batch2-A-part1,fig:perpfrofs-batch2-A-part2,fig:perpfrofs-batch2-B-part1,fig:perpfrofs-batch2-B-part2,fig:perpfrofs-batch2-P-part1,fig:perpfrofs-batch2-P-part2}
will be discussed in the next dedicated \cref{sec:results-discussion}.

\input{Content/Chapters/Include/perfprofs-batch1.tex}
\input{Content/Chapters/Include/perfprofs-batch2.tex}

\subsection{Discussion of the Empirical Results}
\label{sec:results-discussion}

In this section we discuss the empirical results depicted in
\cref{fig:perpfrofs-batch2-E-part1,fig:perpfrofs-batch2-E-part2,fig:perpfrofs-batch2-F-part1,fig:perpfrofs-batch2-F-part2,fig:perpfrofs-batch2-A-part1,fig:perpfrofs-batch2-A-part2,fig:perpfrofs-batch2-B-part1,fig:perpfrofs-batch2-B-part2,fig:perpfrofs-batch2-P-part1,fig:perpfrofs-batch2-P-part2}.
The VRPSolver's labeling algorithm in the empirical results is denoted as \texttt{libRCSP DP pricer},
while our implemented BAC-pricer is denoted as \texttt{BAC MIP Pricer (AFL)}.

We tested the performance of the two pricer approaches for different test sets under progressively higher vehicle capacities.
We tested for different inflation scale factors $s$, namely $s \in \Set*{1, 2, 4, 5, 8, 10, 20}$.
See \cref{sec:inflation-of-the-cvrp-test-instances} for a discussion on how the inflated CVRP instances were obtained.

The empirical results of
\cref{fig:perpfrofs-batch2-E-part1,fig:perpfrofs-batch2-E-part2,fig:perpfrofs-batch2-F-part1,fig:perpfrofs-batch2-F-part2,fig:perpfrofs-batch2-A-part1,fig:perpfrofs-batch2-A-part2,fig:perpfrofs-batch2-B-part1,fig:perpfrofs-batch2-B-part2,fig:perpfrofs-batch2-P-part1,fig:perpfrofs-batch2-P-part2}
are subdivided as follows:
\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{0pt}

	\item The empirical evaluation for the \texttt{E} test set is contained in \cref{fig:perpfrofs-batch2-E-part1,fig:perpfrofs-batch2-E-part2}.
	\item The empirical evaluation for the \texttt{F} test set is contained in \cref{fig:perpfrofs-batch2-F-part1,fig:perpfrofs-batch2-F-part2}.
	\item The empirical evaluation for the \texttt{A} test set is contained in \cref{fig:perpfrofs-batch2-A-part1,fig:perpfrofs-batch2-A-part2}.
	\item The empirical evaluation for the \texttt{B} test set is contained in \cref{fig:perpfrofs-batch2-B-part1,fig:perpfrofs-batch2-B-part2}.
	\item The empirical evaluation for the \texttt{P} test set is contained in \cref{fig:perpfrofs-batch2-P-part1,fig:perpfrofs-batch2-P-part2}.
\end{itemize}

The situation depicted by the empirical results is summarized in the remainder of this section.

\medskip

In the case of the \texttt{E} test set, we can see that at $s \le 4$, the BAC-pricer is not sufficiently competitive but was able to feed slightly better dual bounds to the BPC framework.
The BAC-pricer begins to compete with the labeling algorithm at $s = 5$.
For $s \in \Set*{8, 10, 20}$, not only is the BAC-pricer faster than the labeling algorithm but feeds substantially better dual bounds to the BPC framework.
At $s = 20$, the labeling algorithm is 20 times slower than the BAC-pricer in 42\% of the cases.

In the case of the \texttt{F} test set, we can see that at $s \in \Set*{1, 2}$, the BAC-pricer is not competitive and occasionally fails to complete the pricing problem within the time limit of 20 minutes.
This scenario is most likely caused by the \texttt{F-n135-k7} instance, for which we have empirically observed to require a massive amount of cutting planes, resulting in memory exhaustion for the host machine.
At $s \in \Set*{4, 5, 8, 10, 20}$, the BAC-pricer performs progressively better and feeds tremendously tighter dual bounds to the BPC framework.

In the case of the \texttt{A} test set, the BAC-pricer is not competitive at $s \in \Set*{1, 2, 4}$ and occasionally fails to complete the pricing problem within the time limit.
At $s = 5$, the BAC-pricer begins to be competitive in terms of running time.
The BAC approach yields no discernible dual-bound improvements to the BPC framework.

In the case of the \texttt{B} test set, the BAC-approach becomes competitive at $s = 5$.
At $s \le 4$, it fails to complete the pricing problem within the time limit.
At $s = 20$, the BAC-pricer outperforms the labeling algorithm.
When $s = 10$, the BAC-pricer produces slightly tighter dual-bounds, but significantly better ones when $s = 20$.

In the case of the \texttt{P} test set, the BAC-approach becomes competitive at $s = 5$.
At $s = 8$, the BAC approach outperforms the labeling algorithm while yielding significantly better dual bounds to the BPC framework.
